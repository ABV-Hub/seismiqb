{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizons detection model\n",
    "\n",
    "Seismic horizon is a change in rock properties across a boundary between two layers of rock, particularly seismic velocity and density. Such changes are visible in seismic images (even for an untrained eye), and could be automatically detected. This notebook demonstrates how to build convolutional neural network that detects all of the horizonts on the given cube.\n",
    "\n",
    "* [Dataset](dataset)\n",
    "* [Model architecture](architecture)\n",
    "* [Training](training)\n",
    "* [Validation](validation)\n",
    "* [Criticism](criticism)\n",
    "* [Conclusion](conclusion)\n",
    "* [Suggestions for improvements](suggestions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We use multiple seismic cubes, in particular, `Cube 1`, `Cube 3` and `Cube 4`, for training. `Cube 2` is reserved for validation purposes. A number of hand-labelled horizonts (ranging from 4 to 10) goes along with each cube, and can be seen on image below. Detailed description of each cube, including more sample images, is available [here](./../datasets/Horizonts_modelling.ipynb).\n",
    "\n",
    "<img src=\"images/cube_detection.jpg\" alt=\"Drawing\" style=\"width: 600px; height: 400px\"/>\n",
    "\n",
    "Importantly, each cube is too big to fit into the GPU memory, so we must cut small crops out of it in order to train the model. This changes whole pipeline of research: we need convenient methods of:\n",
    "* splitting seismic cubes into crops of desired shape\n",
    "* lazy loading crops into memory (with quite harsh memory constraints)\n",
    "* training model on labeled crops\n",
    "* predicting with trained model on unlabeled crops\n",
    "* assemble predictions back into the whole cube\n",
    "\n",
    "`Seismiqb` package provides exactly this. Some examples of ready-to-use pipelines can be found [here.](./../tutorials/2.%20Batch.ipynb)\n",
    "\n",
    "**Note:** we heavily rely on [BatchFlow](https://github.com/analysiscenter/batchflow) to define sophisticated neural network architectures with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.append('..')\n",
    "from seismiqb.batchflow import Pipeline, FilesIndex\n",
    "from seismiqb.batchflow import B, V, C, L, F, D, P, R, W\n",
    "from seismiqb.batchflow.models.tf import *\n",
    "from seismiqb.batchflow.models.tf.layers import conv_block\n",
    "from seismiqb import SeismicCropBatch, SeismicGeometry, SeismicCubeset\n",
    "from seismiqb import plot_loss\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Set GPU\n",
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we index `.hdf5` cubes with the help of [FilesIndex](https://analysiscenter.github.io/batchflow/intro/dsindex.html). `SeismicCubeset` provides lots of processing actions for different types of seismic information (cubes in different formats, labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_cubes = ['/notebooks/SEISMIC_DATA/CUBE_1/E_anon.hdf5',\n",
    "               '/notebooks/SEISMIC_DATA/CUBE_3/P_cube.hdf5',\n",
    "               '/notebooks/SEISMIC_DATA/CUBE_4/R_cube.hdf5']\n",
    "\n",
    "dsi = FilesIndex(path=paths_cubes, no_ext=True)\n",
    "ds = SeismicCubeset(dsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get more information about dataset and lay out inner structure. That is done by `load` method of `SeismicCubeset`, that is chain-calling different functions under the hood: one to infer cube geometries, three to parse `.txt` labels and convert them into format with quick access, and, finally, create `Sampler`: that is an entity that allows to generate crop locations exactly where we want them to be. All of these are explained in great details in [this](./../tutorials/1.%20Cubeset.ipynb) tutorial.\n",
    "\n",
    "As we want to train model more on harder data, different proportions for cubes for sampling points from are set: we want to train not as often on Cube 3 due to its simplicity. That is done via `p` (proportions) argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = ds.load(p=[0.4, 0.2, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always helpful to take a look on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show_slide(idx=2, iline=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that we have a whole cube (namely, `Cube 2`) for validation purposes, we would like to hold out  20% of each cube in the train dataset. That can be easily achieved with `modify_sampler` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.modify_sampler('train_sampler', low=0.0, high=0.8, finish=True)\n",
    "ds.modify_sampler('test_sampler',  low=0.8, high=1.0, finish=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That creates two attributes, `ds.train_sampler` and `ds.test_sampler`, which are used to sample crops from the first 80% of ilines or the last 20% of ilines respectively. That is also shown in [tutorial](./../tutorials/1.%20Cubeset.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='architecture'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "We use convonlutional neural network in `EncoderDecoder` fashion and train on crops of (256, 256, 2) size:\n",
    "* First of all, initial crop is downsampled twice along xlines/heights dimensions, effectively reducing resolution 4 times\n",
    "* Then, we encode crop by applying `inception_a_block` of Inception_v4 3 times with max-pooling in-between\n",
    "* `Inception_c_block` is applied at the bottleneck\n",
    "* Initial shape of the block is restored by transposed convolutions with ordinary convolutions+batchnorm+activation(ReLU) in-between\n",
    "\n",
    "Multiple things are worth noting. Most importantly, model percieves every crop (a 3-d entity) as sequence of 2-d images stacked one after the other, and every convolution that is used is 2D.\n",
    "Every max-pooling in the network is of size and stride 2, effectively halving the resolution of its inputs. To get precise definitions of `Inception` blocks, check [this](https://arxiv.org/pdf/1602.07261.pdf) paper.\n",
    "\n",
    "Technical note: in order to compute `Dice`-coefficient, we need to add axis both to the output of neural network and to initial labels. That is done via `predictions` callable in the first case and `add_axis` action in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "EPOCHS = 1000\n",
    "NUM_CROPS = 64\n",
    "CROP_SHAPE = (2, 256, 256) # i, x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ED class is to ensure that `head` block does not `crop` its inputs.\n",
    "# For more on that, check the `head` method of EncoderDecoder\n",
    "class ED(EncoderDecoder):\n",
    "    @classmethod\n",
    "    def head(cls, inputs, targets, name='head', **kwargs):\n",
    "        kwargs = cls.fill_params('head', **kwargs)\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            x = TFModel.head(inputs, name, **kwargs)\n",
    "            channels = cls.num_channels(targets)\n",
    "            if cls.num_channels(x) != channels:\n",
    "                args = {**kwargs, **dict(layout='c', kernel_size=1, filters=channels)}\n",
    "                x = conv_block(x, name='conv1x1', **args)\n",
    "                x = tf.expand_dims(x, axis=-1, name='expand')\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    # Shapes and orders\n",
    "    'inputs/images/shape': (None, None, CROP_SHAPE[0]),\n",
    "    'inputs/masks/shape': (None, None, CROP_SHAPE[0], 1), \n",
    "    'initial_block/inputs': 'images',\n",
    "    'common/data_format': 'channels_last',\n",
    "    # Model layout\n",
    "    'initial_block': {'layout': 'pp'},\n",
    "    'body/encoder': {'num_stages': 3,\n",
    "                     'blocks': {'base': Inception_v4.inception_a_block,\n",
    "                                'filters': [[32, 16], [48, 32], [64, 48]]}},\n",
    "    'body/embedding': {'base': Inception_v4.inception_c_block,\n",
    "                       'filters': [32, 48, 64, 96]},\n",
    "    'body/decoder': {'num_stages': 5,\n",
    "                     'blocks': {'layout':'cna',\n",
    "                                'filters': [32, 16, 8, 6, 4]}},\n",
    "    'output': 'sigmoid',\n",
    "    # Train configuration\n",
    "    'loss': 'dice',\n",
    "    'optimizer': 'Adam',\n",
    "    'decay': {'name': 'invtime',\n",
    "              'learning_rate': 0.01,\n",
    "              'decay_rate': 1,\n",
    "              'decay_steps': 100},\n",
    "    'microbatch': 4,\n",
    "}\n",
    "\n",
    "pipeline_config = {\n",
    "    'model': ED,\n",
    "    'model_config': model_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Neural network is trained on crops of fixed shape. Pipeline consists of following steps:\n",
    "* First of all, we initialize all the pipeline variables, that we need, and the model itself\n",
    "* Then, we create positions of crops, then load actual data and labels for it. Data is immediately scaled to $[0, 1]$ range in order to normalize values from different cubes\n",
    "* Right after, multiple augmentations are applied to simulate different distortions and make model robust to them\n",
    "* Model weights update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = (Pipeline(config=pipeline_config)\n",
    "                  # Initialize pipeline variables and model\n",
    "                  .init_variable('loss_history', [])\n",
    "                  .init_model('dynamic', C('model'), 'ED', C('model_config'))\n",
    "                  # Load data/masks\n",
    "                  .load_component(src=[D('geometries'), D('labels')],\n",
    "                                  dst=['geometries', 'labels'])\n",
    "                  .crop(points=D('train_sampler')(NUM_CROPS),\n",
    "                        shape=CROP_SHAPE)\n",
    "                  .load_cubes(dst='images')\n",
    "                  .create_masks(dst='masks', width=2)\n",
    "                  .rotate_axes(src=['images', 'masks'])\n",
    "                  .scale(mode='normalize', src='images')\n",
    "                  # Augmentations\n",
    "                  .additive_noise(scale=0.005,\n",
    "                                  src='images', dst='images', p=0.2)\n",
    "                  .rotate(angle=P(R('uniform', -30, 30)),\n",
    "                          src=['images', 'masks'], p=0.4)\n",
    "                  .scale_2d(scale=P(R('uniform', 0.7, 1.3)),\n",
    "                            src=['images', 'masks'], p=0.4)\n",
    "                  .elastic_transform(alpha=P(R('uniform', 35, 45)), sigma=P(R('uniform', 4, 4.5)),\n",
    "                                     src=['images', 'masks'], p=0.2)\n",
    "                  # Training\n",
    "                  .add_axis(src='masks', dst='masks')\n",
    "                  .train_model('ED', \n",
    "                               fetches='loss',\n",
    "                               images=B('images'),\n",
    "                               masks=B('masks'),\n",
    "                               save_to=V('loss_history', mode='a')))\n",
    "\n",
    "# Add dataset to pipeline\n",
    "train_pipeline = train_template << ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch contains 64 crops of (256, 256, 2) size. Model is trained for 1500 epochs by `Adam` optimizer with default parameters, [inverse-time learning rate decay](https://www.tensorflow.org/api_docs/python/tf/train/inverse_time_decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop. Allows to see progress (value of loss)\n",
    "train_pipeline.run(D('size'), n_iters=EPOCHS, bar=True,\n",
    "                   bar_desc=W(V('loss_history')[-1].format('Loss is: {:7.7}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss against iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_pipeline.v('loss_history'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, loss starts to plateau after just 90 minutes of training. Relatively high variance suggests that model can benefit from bigger batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='validation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Now, we want to check performance of our model on unseen part of the cubes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation pipeline: no augmentations\n",
    "val_template = (Pipeline()\n",
    "                # Import model\n",
    "                .import_model('ED', train_pipeline)\n",
    "                # Load data/masks\n",
    "                .load_component(src=[D('geometries'), D('labels')],\n",
    "                                dst=['geometries', 'labels'])\n",
    "                .crop(points=D('test_sampler')(4), shape=CROP_SHAPE)\n",
    "                .load_cubes(dst='images')\n",
    "                .create_masks(dst='masks')\n",
    "                .rotate_axes(src=['images', 'masks'])\n",
    "                .scale(mode='normalize', src='images')\n",
    "                # Predict with model\n",
    "                .add_axis(src='masks', dst='masks')\n",
    "                .predict_model('ED', \n",
    "                               fetches='predictions',\n",
    "                               images=B('images'),\n",
    "                               save_to=B('predictions'))\n",
    "                )\n",
    "val_pipeline = val_template << ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = val_pipeline.next_batch(D('size'), n_epochs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just plot images of crops, hand-labeled masks and model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch.plot_components('images', 'masks', 'predictions', overlap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = '/notebooks/SEISMIC_DATA/SAVED/MODELS/ED_DEMO'\n",
    "\n",
    "train_pipeline.save_model_now('ED', model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is way easier to analyse results when multiple crops are aggregated back into bigger picture. Method `make_grid`, paired with action `assemble_crops`, does exactly that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config = {'load/path': model_save_dir}\n",
    "\n",
    "# Pipeline template. Can be used multiple times (for different datasets)\n",
    "val_template = (Pipeline()\n",
    "                # Initialize everything\n",
    "                .init_variable('result_images', [])\n",
    "                .init_variable('result_preds', [])\n",
    "                .init_model('dynamic', TFModel, 'loaded_model', load_config)              \n",
    "                # Load data\n",
    "                .load_component(src=D('geometries'), dst='geometries')\n",
    "                .crop(points=D('grid_gen')(),\n",
    "                      shape=CROP_SHAPE)\n",
    "                .load_cubes(dst='images')\n",
    "                .rotate_axes(src='images')\n",
    "                .scale(mode='normalize', src='images')\n",
    "                # Predict with model, then aggregate\n",
    "                .predict_model('loaded_model', \n",
    "                               fetches=['images', 'predictions'],\n",
    "                               images=B('images'),\n",
    "                               save_to=[V('result_images', mode='e'),\n",
    "                                        V('result_preds', mode='e')])\n",
    "                .assemble_crops(src=V('result_preds'), dst='assembled_pred',\n",
    "                                  grid_info=D('grid_info'))\n",
    "                .assemble_crops(src=V('result_images'), dst='assembled_img',\n",
    "                                  grid_info=D('grid_info'))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = ds.make_grid(ds.indices[0], CROP_SHAPE, \n",
    "                  [2201, 2202], [0, 800], [100, 1300],\n",
    "                  strides=[2, 128, 128])\n",
    "\n",
    "val_pipeline = val_template << ds\n",
    "\n",
    "for _ in trange(ds.grid_iters):\n",
    "    batch = val_pipeline.next_batch(D('size'), n_epochs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.plot_components('assembled_img', 'assembled_pred', idx=None, order_axes=(1, 2, 0), overlap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `assemble_crops` takes action only once, when all of the crops are passed through the model. Also, this function can be quite memory intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to slow changes in data along ilines in any given cube, it might be a good idea to test our model against completely new cube. To begin with, we need to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_path_data = '/notebooks/SEISMIC_DATA/CUBE_2/M_cube.hdf5'\n",
    "\n",
    "test_dsi = FilesIndex(path=[test_path_data], no_ext=True)\n",
    "test_ds = SeismicCubeset(test_dsi)\n",
    "\n",
    "test_ds = test_ds.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_ds = test_ds.make_grid(test_ds.indices[0], CROP_SHAPE, \n",
    "                            [201, 202], [0, 800], [100, 1300],\n",
    "                            strides=[2, 128, 128])\n",
    "\n",
    "val_pipeline = val_template << test_ds\n",
    "\n",
    "for _ in trange(test_ds.grid_iters):\n",
    "    batch = val_pipeline.next_batch(D('size'), n_epochs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.plot_components('assembled_img', 'assembled_pred', idx=None, order_axes=(1, 2, 0), overlap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, even on completely new cube our model outputs reasonable horizonts.\n",
    "\n",
    "Currently, our prediction is essentially an image. In order to compare it with the ground truth, we need to convert it into the same format. That is what `get_point_cloud` function is for: it allows to pick only the highlighted parts of the image as separate horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = test_ds.get_point_cloud(batch.assembled_pred, 'horizons',\n",
    "                            coordinates='lines', threshold=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare model detected horizon to the human-labeled: important metrics are mean/std value of difference and area of horizon with less than 5ms error.\n",
    "\n",
    "**Note:** in the following output `FIRST` stands for the predicted horizon, while `SECOND` denotes the hand-labeled one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_ds.compare_to_labels(test_ds.horizons[0], idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ill-advised to use model to predict on entire seismic cube. To overcome this, we've prepared quite a few of useful [scripts](./../scripts/): one for horizon picking, and one for computing all the metrics presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='criticism'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criticism\n",
    "\n",
    "The task in hand is ill-defined: it is very unclear, which horizons on the slide we want to get, how many of them, with which rules of picking. Current labels are quite inconsistent: they have different phase (some of them are on maximum values of amplitude, some of them at minimums); they separate different objects: most of them are following the brightest line on the slide, some of them are between crucial seismic facies, few of them track fissures. Most of them are made by automatic autocorellation and not really interesting (nor hard).\n",
    "\n",
    "Despite that, this model serves as a great trampoline for the others: [horizont extension](./Horizons_extension.ipynb) and [facies segmentation](./Segmenting_interlayers.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook shows how to build a fully convolutional neural network in order to predict locations of seismic horizons. Dataset, processing pipeline and model training procedure are presented.\n",
    "\n",
    "In exactly the same manner we trained models on shrinked datasets:  we trained one model on each individual cube, using every 200-th iline as the only training data (totalling in no more that 15 slides as a trainset). Unsurprisingly, neural network is able to recover horizons quite well on the rest of the cube: that is due to the fact that cube is changing slowly along its directions. Again, key metrics are area of detected horizon (compared to the hand-labeled one), mean difference between horizons, area of detected horizon that is closer than 5ms to the ground truth:\n",
    "\n",
    "| Train/test cube |   Area, % | Mean error, ms | Area in 5ms window, % |\n",
    "| :------ | ----: | ----: | ----: |\n",
    "| CUBE_1 | 90, 91, 86  | 1.5, 2.1, 4.4 | 96, 94, 85 |\n",
    "| CUBE_2 | 100, 98, 95 | 2.4, 2.1, 2.8 | 96, 99.9, 96.4 |\n",
    "| CUBE_3 | 92, 92, 92  | 1.8, 1.9, 2.5 | 99.8, 99.5, 91.6 | \n",
    "| CUBE_4 |  73, 83, 84 | 3, 3.6, 4.3 | 89, 86, 84 |\n",
    "\n",
    "Results in the table are consistent with our observations: detected horizonts are detected well, and the main point of improvement lies in enlarging the covered area. The only thing that catches the eye is suspiciously low area of detected horizons on `Cube_1`. Returning to the dataset [description](./../datasets/Horizonts_modelling.ipynb), we can easily identify the roots of the problem: hand-labeled horizons are for some reasons present in the zero-traces, and that skews our results.\n",
    "\n",
    "Models, trained only on one cube, hardly works on the others: it just can'd adjust to the altering of values in the traces. Thus, we need to train model on at least multiple cubes in order to generalize on the others. \n",
    "\n",
    "Having 3 cubes in total, we trained models for each pair of them and used it as predictor on the remaining one: results (with the same metrics as before) are shown in the table:\n",
    "\n",
    "| NOT in the training |   Area, % | Mean error, ms | Area in 5ms window, % |\n",
    "| :------ | ----: | ----: | ----: |\n",
    "| CUBE_1 | 5  | 0.85 | 94 |\n",
    "| CUBE_3 | 45  | 1 | 97 | \n",
    "| CUBE_4 |  25 | 50 | 0 |\n",
    "\n",
    "Again, results are not surprising: where test-cube structure resembles the train ones, model can follow the horizon quite well, with the easiest cube being labeled the best. It is the covered area that is the problem: different types of inner noises does not allow model to generalize well enough on big distances. The hardest cube (`Cube_4`) stays pretty much unlabeled due to its unique hardness.\n",
    "\n",
    "Overall, we can see that getting more data is of utmost importance: it helps to generalize better on unseen cubes in order to cover bigger area with predictions. Despite that, even this simple model, trained on just a few cubes, performs reasonably well and can be further improved by using deeper architecture and finer choice of hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='suggestions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggestions for improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thorough research of crop shape influence is needed: it is unknown which crop shape is the best, how dynamic shape changes model performance etc.\n",
    "\n",
    "Also, this model can be used to get more train data for the others: for example, we can use it to predict somewhat continious horizons and fed it to `extension` model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
