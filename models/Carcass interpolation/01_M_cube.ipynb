{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carcass interpolation model\n",
    "\n",
    "Seismic horizon is a change in rock properties across a boundary between two layers of rock, particularly seismic velocity and density. Such changes are visible in seismic images (even for an untrained eye), and could be automatically detected. \n",
    "\n",
    "A possible approach to the task of semi-automatic horizon detection is to track reflection by hand of a very sparse grid and use a neural network to interpolate it on the whole seismic (spatial) range. This notebooks shows how to do it in details.\n",
    "\n",
    "* [Dataset](dataset)\n",
    "* [Model architecture](architecture)\n",
    "* [Training](training)\n",
    "* [Validation](validation)\n",
    "* [Criticism](criticism)\n",
    "* [Conclusion](conclusion)\n",
    "* [Suggestions for improvements](suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append('../../seismiqb')\n",
    "from seismiqb.batchflow import Pipeline, FilesIndex\n",
    "from seismiqb.batchflow import B, V, C, F, D, P, R, W\n",
    "from seismiqb.batchflow.models.torch import EncoderDecoder, ResBlock\n",
    "\n",
    "from seismiqb import SeismicCubeset, Horizon, HorizonMetrics\n",
    "from seismiqb import plot_image, plot_loss\n",
    "\n",
    "# Set GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "FREQUENCIES = [100, 300]         # carcass frequency at `hard` and `easy` locations\n",
    "CROP_SHAPE = (1, 256, 256)       # shape of sampled 3D crops\n",
    "ITERS = 350                      # number of train iterations\n",
    "BATCH_SIZE = 64                  # number of crops inside one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "# Dataset\n",
    "\n",
    "We use only one cube and one horizon at a time. The next cell loads them into one entity `dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_path = '/data/seismic/CUBE_2/M_cube.hdf5'\n",
    "horizon_path = '/data/seismic/CUBE_2/RAW/t0_B_anon'\n",
    "\n",
    "dsi = FilesIndex(path=[cube_path], no_ext=True)\n",
    "dataset = SeismicCubeset(dsi)\n",
    "\n",
    "dataset.load_geometries()\n",
    "dataset.create_labels({dataset.indices[0]: [horizon_path]})\n",
    "\n",
    "geometry = dataset.geometries[0]\n",
    "horizon = dataset.labels[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always nice to look at you data: the next cells show horizon depth map, as well as an example of seismic slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon.show_slide(100, axis=0, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be great to have a tool to assess quality of the horizon. What would be even better is to know exact locations, where the horizon is good (meaning that the reflection is tracked with the agreement with seismic and the phase of tracking is relatively not changing much), and where it can be further improved.\n",
    "\n",
    "Furtunately, we provide such a tool! By the virtue of `HorizonMetrics`, we can compute maps that assess quality of each spatial point and provide key insight about labeled surface. To learn more about how they are created, check our [metrics notebook](sda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate against seismic data\n",
    "hm = HorizonMetrics(horizon)\n",
    "\n",
    "corrs = hm.evaluate('support_corrs', agg='nanmean', supports=50, plot=True)\n",
    "cross = hm.evaluate('support_crosscorrs', agg='median', supports=50, plot=True,\n",
    "                    zmin=-3, zmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train only on a sparse grid of labels and, obviously, there are a lot of ways of creating such a grid. Our approach is to locate  cube anomalies (you can learn more about that in [metrics notebook](asd)), and then create grid corresponding to the local geological hardness of the data: the carcass is finer at the harder places, while there is no need for it in other, relatively easier places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create carcass to train on\n",
    "quality_grid = geometry.make_quality_grid(FREQUENCIES)\n",
    "grid_coverage = (np.nansum(geometry.quality_grid) /\n",
    "                 (np.prod(geometry.cube_shape[:2]) - np.nansum(geometry.zero_traces)))\n",
    "\n",
    "print(f'Grid covers {grid_coverage:.2} of the cube spatial ranges')\n",
    "plot_image(quality_grid, title=f'Quality grid on {geometry.short_name}',\n",
    "           xlabel='ilines', ylabel='xlines',\n",
    "           cmap='Reds', interpolation='bilinear', rgb=True, figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are very few slices left! Note that some of them are `inline`-oriented, some of them are `xline`-oriented: our framework allows to dynamically sample slices in different directions.\n",
    "\n",
    "As usual, we train the model on 3D crops of data cut from the cube. We need a mechanism of sampling such crops, and that is exactly what `Sampler` is doing. Output of the next cell shows actual sampled slices, that (potentially) can be used at training time. Color corresponds to frequency of the spatial point appearing in the batch.\n",
    "\n",
    "***Note the `adaptive` slices: that is the parameter that tells our library to sample points only from the grid.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampler, according to carcass\n",
    "dataset.create_sampler(quality_grid=True)\n",
    "dataset.modify_sampler('train_sampler', finish=True)\n",
    "\n",
    "_ = dataset.show_slices(src_sampler='train_sampler',\n",
    "                        normalize=False, shape=CROP_SHAPE,\n",
    "                        adaptive_slices=True,\n",
    "                        cmap='Reds', interpolation='bilinear',\n",
    "                        figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, lets create testing sampler, that is not connected to the grid: that allows us to later use model on other parts of the cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.create_sampler(quality_grid=False)\n",
    "dataset.modify_sampler('test_sampler', finish=True)\n",
    "\n",
    "_ = dataset.show_slices(src_sampler='test_sampler',\n",
    "                        normalize=False, shape=CROP_SHAPE,\n",
    "                        adaptive_slices=False,\n",
    "                        cmap='Reds', interpolation='bilinear',\n",
    "                        figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='architecture'></a>\n",
    "# Model architecture\n",
    "\n",
    "We use convolutional neural network in `hourglass` manner, that consists of following parts:\n",
    "\n",
    "- Initial processing via [`ResNet`](https://arxiv.org/abs/1512.03385) blocks\n",
    "- Backbone to encode the image: it is very similar of `ResNet` and downsamples the input by a factor of 16\n",
    "- Embedding layer\n",
    "- Decoder, that takes output of the embedding and upsamples it to initial size, using pre-stored skip-connections to include informations from an earlier layers\n",
    "\n",
    "We optimize Dice-coefficient with `Adam` algorithm. Due to enormous size of crops, we split every batch into pieces (called microbatches), update gradient from every of them and only then update neural network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_loss(pred, target, eps=1e-7):\n",
    "    num_classes = pred.shape[1]\n",
    "    target = target.long()\n",
    "    if num_classes == 1:\n",
    "        true_1_hot = torch.eye(num_classes + 1)[target.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
    "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
    "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "        pos_prob = torch.sigmoid(pred)\n",
    "        neg_prob = 1 - pos_prob\n",
    "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
    "    else:\n",
    "        true_1_hot = torch.eye(num_classes)[target.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        probas = F.softmax(pred, dim=1)\n",
    "\n",
    "    true_1_hot = true_1_hot.to(pred.device).type(pred.type())\n",
    "    dims = (0,) + tuple(range(2, target.ndimension()))\n",
    "    intersection = torch.sum(probas * true_1_hot, dims)\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
    "    loss = (2. * intersection / (cardinality + eps)).mean()\n",
    "    return 1 - loss\n",
    "\n",
    "\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    # Model layout\n",
    "    'initial_block': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': 16,\n",
    "        'kernel_size': 5,\n",
    "        'downsample': False,\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "\n",
    "    'body/encoder': {\n",
    "        'num_stages': 4,\n",
    "        'order': 'sbd',\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'n_reps': 1,\n",
    "            'filters': [32, 64, 128, 256],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'body/embedding': {\n",
    "        'base': ResBlock,\n",
    "        'n_reps': 1,\n",
    "        'filters': 256,\n",
    "        'attention': 'scse',\n",
    "    },\n",
    "    'body/decoder': {\n",
    "        'num_stages': 4,\n",
    "        'upsample': {\n",
    "            'layout': 'tna',\n",
    "            'kernel_size': 2,\n",
    "        },\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'filters': [128, 64, 32, 16],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'head': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': [16, 8],\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "    'output': 'sigmoid',\n",
    "    # Train configuration\n",
    "    'loss': dice_loss,\n",
    "    'optimizer': {'name': 'Adam', 'lr': 0.01,},\n",
    "    \"decay\": {'name': 'exp', 'gamma': 0.1},\n",
    "    \"n_iters\": 150,\n",
    "    'microbatch': 4,\n",
    "    'common/activation': 'relu6',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "# Training\n",
    "\n",
    "Pipeline of training consists of following steps:\n",
    "\n",
    "- First of all, we create locations of crops to cut (`crop` action)\n",
    "- Then we create masks, load actual seismic data\n",
    "- As some of the crops are `iline`-oriented, and some of them are `xline`-oriented, we need to force equal shape on all of them. That is what `adaptive_reshape` for!\n",
    "- We normalize seismic data by dividing by 99 quantile of the data in that cube: this way, most of the data is in $[-1, 1]$ range\n",
    "- We apply loads of augmentations: after all, our training dataset consists of just a few seismic slices! We are enriching them by using flips, rotations, zoom, elastic transforms.\n",
    "\n",
    "***Note that we use `transpose` before and after augmentations: turns out, that it is faster to make our crop look like an image and use highly optimized `OpenCV` routines, than to apply transformations along 'unadvantageous' axes.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = (\n",
    "    Pipeline()\n",
    "    # Initialize pipeline variables and model\n",
    "    .init_variable('loss_history', [])\n",
    "    .init_model('dynamic', EncoderDecoder, 'model', MODEL_CONFIG)\n",
    "\n",
    "    # Load data/masks\n",
    "    .crop(points=D('train_sampler')(BATCH_SIZE),\n",
    "          shape=CROP_SHAPE, adaptive_slices=True)\n",
    "    .create_masks(dst='masks', width=5)\n",
    "    .load_cubes(dst='images')\n",
    "    .adaptive_reshape(src=['images', 'masks'], shape=CROP_SHAPE)\n",
    "    .scale(mode='q', src='images')\n",
    "\n",
    "    # Augmentations\n",
    "    .transpose(src=['images', 'masks'], order=(1, 2, 0))\n",
    "    .flip(axis=1, src=['images', 'masks'], seed=P(R('uniform', 0, 1)), p=0.3)\n",
    "    .additive_noise(scale=0.005, src='images', dst='images', p=0.3)\n",
    "    .rotate(angle=P(R('uniform', -15, 15)),\n",
    "            src=['images', 'masks'], p=0.3)\n",
    "    .scale_2d(scale=P(R('uniform', 0.85, 1.15)),\n",
    "              src=['images', 'masks'], p=0.3)\n",
    "    .elastic_transform(alpha=P(R('uniform', 35, 45)),\n",
    "                       sigma=P(R('uniform', 4, 4.5)),\n",
    "                       src=['images', 'masks'], p=0.2)\n",
    "    .transpose(src=['images', 'masks'], order=(2, 0, 1))\n",
    "\n",
    "    # Training\n",
    "    .train_model('model',\n",
    "                 fetches='loss',\n",
    "                 images=B('images'),\n",
    "                 masks=B('masks'),\n",
    "                 save_to=V('loss_history', mode='a'))\n",
    ")\n",
    "\n",
    "train_pipeline = train_template << dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_pipeline.run(D('size'), n_iters=ITERS, bar='n',\n",
    "                   bar_desc=W(V('loss_history')[-1].format('Loss is: {:7.7}')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_pipeline.v('loss_history'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that loss platoed rather quickly. High fluctuations suggest that using larger batch size may be of help, as well as the more agressive learning rate scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='validation'></a>\n",
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model on previously unseen data: note the usage of `test_sampler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation pipeline: no augmentations\n",
    "val_template = (\n",
    "    Pipeline()\n",
    "    # Import model\n",
    "    .import_model('model', train_pipeline)\n",
    "\n",
    "    # Load data/masks\n",
    "    .crop(points=D('test_sampler')(4), shape=CROP_SHAPE)\n",
    "    .load_cubes(dst='images')\n",
    "    .create_masks(dst='masks')\n",
    "    .scale(mode='q', src='images')\n",
    "\n",
    "    # Predict with model\n",
    "    .predict_model('model',\n",
    "                   B('images'),\n",
    "                   fetches='predictions',\n",
    "                   save_to=B('predictions'))\n",
    "    .transpose(src=['images', 'masks', 'predictions'],\n",
    "               order=(1, 2, 0))\n",
    ")\n",
    "val_pipeline = val_template << dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = val_pipeline.next_batch(D('size'), n_epochs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_idx = 0\n",
    "batch.plot_components('images', 'masks', 'predictions', mode='separate', idx=b_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that model easily tracks the horizon.\n",
    "\n",
    "Now we can save the trained model for later usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = '/data/seismic/SAVED/MODELS/demo_carcass'\n",
    "\n",
    "train_pipeline.save_model_now('model', model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now is the perfect time to use our model to reconstruct the whole horizon: we can do so by:\n",
    "- splitting the cube into chunks that cover the entire seismic range\n",
    "- for each of them, makea prediction with the model\n",
    "- aggregate predictions into one giant 3D array\n",
    "- extract the horizon surface from it\n",
    "\n",
    "Next cells do exactly that: `make_grid` creates grid of crop-sized chunks, `inference_pipeline` is used to make predictions, and `from_mask` method of `Horizon` allows us to locate the surface inside 3D array.\n",
    "\n",
    "***Note that `from_mask` is more that capable of detecting and extracting multiple horizons from the volume. In the task of horizon interpolation we expect only one surface, so we take only the biggest of the produced horizons for later evaluation.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_template = (\n",
    "    Pipeline()\n",
    "    # Initialize everything\n",
    "    .init_variable('result_preds', [])\n",
    "    .import_model('model', train_pipeline)\n",
    "\n",
    "    # Load data\n",
    "    .crop(points=D('grid_gen')(),\n",
    "          shape=CROP_SHAPE)\n",
    "    .load_cubes(dst='images')\n",
    "    .adaptive_reshape(src='images', shape=CROP_SHAPE)\n",
    "    .scale(mode='q', src='images')\n",
    "\n",
    "    # Predict with model, then aggregate\n",
    "    .predict_model('model',\n",
    "                   B('images'),\n",
    "                   fetches='predictions',\n",
    "                   save_to=V('result_preds', mode='e'))\n",
    "    .assemble_crops(src=V('result_preds'),\n",
    "                    dst='assembled_pred',\n",
    "                    grid_info=D('grid_info'),\n",
    "                    order=(0, 1, 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_grid(dataset.indices[0], CROP_SHAPE,\n",
    "                  [0, 417], [0, 868], [800, 1000],\n",
    "                  strides=(1, 96, 96),\n",
    "                  batch_size=BATCH_SIZE*2)\n",
    "\n",
    "inference_pipeline = inference_template << dataset\n",
    "\n",
    "for _ in tqdm(range(dataset.grid_iters)):\n",
    "    batch = inference_pipeline.next_batch(D('size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = Horizon.from_mask(batch.assembled_pred, dataset.grid_info,\n",
    "                             minsize=50, threshold=0.5)\n",
    "\n",
    "predicted_horizon = horizons[-1] # the biggest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_horizon.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are some rough pathes at the edges, but the overwhelming majority of points is labeled.\n",
    "\n",
    "Now we apply the same evaluating procedure, as was applied to the target horizon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate against seismic data\n",
    "hm = HorizonMetrics(predicted_horizon)\n",
    "\n",
    "corrs = hm.evaluate('support_corrs', agg='nanmean', supports=50, plot=True)\n",
    "cross = hm.evaluate('support_crosscorrs', agg='median', supports=50, plot=True,\n",
    "                    zmin=-3, zmax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the quality of tracking is even better, than it was in the original horizon! That is because the model learned on very small amount of slices, where tracked phase was relatively unchanged, and picked the same phase on the rest of the cube points.\n",
    "\n",
    "Finally, we compare prediction and target horizon head-to-head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate against target horizon\n",
    "dataset.compare_to_labels(predicted_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='criticism'></a>\n",
    "# Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear whether the model is too big for the task at hand: there is a chance, that the same results can be obtained with way smaller neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now, the model is more than capable of extending a sparse carcass to a whole spatial range of the cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='suggestions'></a>\n",
    "# Suggestion for improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed research on hyperparameters should be performed: model architecture can be chosen better, as well as `crop_shape` that has a huge influence on the quality of predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
