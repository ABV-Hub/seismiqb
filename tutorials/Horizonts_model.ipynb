{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network to detect horizonts\n",
    "\n",
    "Welcome! It is about time to create our ML model to automatically detect horizonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import segyio\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "\n",
    "sys.path.append('..')\n",
    "from seismiqb.batchflow import Dataset, Pipeline, FilesIndex\n",
    "from seismiqb.batchflow import B, V, C, L, F, D, P, R\n",
    "from seismiqb.batchflow.models.tf import UNet, TFModel, DenseNet\n",
    "from seismiqb.batchflow.models.tf.layers import conv_block\n",
    "from seismiqb import SeismicCropBatch, SeismicGeometry, SeismicCubeset\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_0 = '/notebooks/SEISMIC_DATA/CUBE_1/E_anon.hdf5'\n",
    "path_data_1 = '/notebooks/SEISMIC_DATA/CUBE_3/P_cube.hdf5'\n",
    "path_data_2 = '/notebooks/SEISMIC_DATA/CUBE_VUONGMK/Repaired_cube.hdf5'\n",
    "\n",
    "dsi = FilesIndex(path=[path_data_0, path_data_1, path_data_2], no_ext=True)\n",
    "ds = SeismicCubeset(dsi)\n",
    "\n",
    "ds = (ds.load_geometries()\n",
    "        .load_point_clouds(path = path_pc_saved)\n",
    "        .load_labels()\n",
    "        .load_samplers(p=[0.4, 0.2, 0.4])\n",
    "      )\n",
    "\n",
    "# ~80 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all of the constants at one place\n",
    "**Note:** to use Dice-coefficient as loss function, we need to add axis to masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAPES\n",
    "EPOCHS = 100\n",
    "NUM_CROPS = 16\n",
    "CROP_SHAPE = [2, 256, 256]                              # i, x, h\n",
    "MODEL_SHAPE = CROP_SHAPE[-2:] + [CROP_SHAPE[0]]         # x, h, i\n",
    "MODEL_SHAPE_DICE = MODEL_SHAPE + [1]\n",
    "\n",
    "MODEL_SHAPE = tuple(MODEL_SHAPE)\n",
    "MODEL_SHAPE_DICE = tuple(MODEL_SHAPE_DICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "We use smaller version of [one hundred layer Tiramisu](https://arxiv.org/abs/1611.09326) as our base model.\n",
    "\n",
    "As noted before, we add axis to the output of neural network in order to correctly compute Dice-coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(x):\n",
    "    return tf.expand_dims(x, axis=-1, name='expand')\n",
    "\n",
    "# DenseNet config\n",
    "model_config_dense = {\n",
    "                    'inputs': dict(cubes={'shape': MODEL_SHAPE},\n",
    "                                   masks={'name': 'targets', 'shape': MODEL_SHAPE_DICE}), \n",
    "                    'initial_block/inputs': 'cubes',\n",
    "                    'body': {'num_layers': [2]*3,\n",
    "                             'block/growth_rate': 8},\n",
    "                    'loss': 'dice',\n",
    "                    'optimizer': 'Adam',\n",
    "                    'predictions': predictions,\n",
    "                    'output': 'sigmoid',\n",
    "                    'common': {'data_format': 'channels_last'}\n",
    "                     }\n",
    "\n",
    "pipeline_config = {'model': DenseNetFC,\n",
    "                   'model_config': model_config_dense}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline of training:\n",
    "* create positions to cut data from. Note that we use `truncate` method of sampler to sample only from first 80% of ilines\n",
    "* load data from cubes, create segmentation masks\n",
    "* some quality of life augmentations: rotate axis (so we can think of our crop as of image), scale to force cubes to have values in the same range\n",
    "* start training!\n",
    "\n",
    "**Note:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = (Pipeline(config=pipeline_config)\n",
    "                  .load_component(src=[D('geometries'), D('labels')],\n",
    "                                  dst=['geometries', 'labels'])\n",
    "                  .crop(points=L(ds.sampler.truncate(high=0.8, expr=lambda p: p[:, 1]).sample, NUM_CROPS), shape=CROP_SHAPE)\n",
    "                  .load_cubes(dst='data_crops')\n",
    "                  .load_masks(dst='mask_crops')\n",
    "                  .apply_transform(rotate_axis, src=['data_crops', 'mask_crops'], dst=['data_crops', 'mask_crops'])\n",
    "                  .scale(mode='normalize', src='data_crops')\n",
    "                  # Training\n",
    "                  .apply_transform(add_axis, src='mask_crops', dst='mask_crops')\n",
    "                  .init_variable('loss_history', init_on_each_run=list)\n",
    "                  .init_variable('current_loss')\n",
    "                  .init_model('dynamic', C('model'), 'Dense', C('model_config'))\n",
    "                  .train_model('Dense', \n",
    "                               fetches='loss',\n",
    "                               make_data={'cubes': B('data_crops'), 'masks': B('mask_crops')},\n",
    "                               save_to=V('loss_history'), mode='a')) << ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline.run(3, n_epochs=EPOCHS, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how our model performs on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_pipeline.get_variable('loss_history'))\n",
    "plt.xlabel(\"Iterations\"), plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance on unseen part of the cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to slow changes in data along ilines in any given cube, it might be a good idea to test our model against completely new cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
