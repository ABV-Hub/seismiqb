{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network to detect horizonts\n",
    "\n",
    "Welcome! It is about time to create our ML model to automatically detect horizonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary modules\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from seismiqb.batchflow import Dataset, Pipeline, FilesIndex\n",
    "from seismiqb.batchflow import B, V, C, L, F, D, P, R\n",
    "from seismiqb.batchflow.models.tf import DenseNet\n",
    "from seismiqb import SeismicCropBatch, SeismicGeometry, SeismicCubeset\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, we create `SeismicCubeset`\n",
    "To learn more what it is all about, check out our previous tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_0 = '/notebooks/SEISMIC_DATA/CUBE_1/E_anon.hdf5'\n",
    "path_data_1 = '/notebooks/SEISMIC_DATA/CUBE_3/P_cube.hdf5'\n",
    "path_data_2 = '/notebooks/SEISMIC_DATA/CUBE_VUONGMK/Repaired_cube.hdf5'\n",
    "\n",
    "dsi = FilesIndex(path=[path_data_0, path_data_1, path_data_2], no_ext=True)\n",
    "ds = SeismicCubeset(dsi)\n",
    "\n",
    "ds = (ds.load_geometries()\n",
    "        .load_point_clouds(path = path_pc_saved)\n",
    "        .load_labels()\n",
    "        .load_samplers(p=[0.4, 0.2, 0.4])\n",
    "      )\n",
    "\n",
    "# ~80 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all of the constants at one place\n",
    "**Note:** to use Dice-coefficient as loss function, we need to add axis to masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAPES\n",
    "EPOCHS = 100\n",
    "NUM_CROPS = 16\n",
    "CROP_SHAPE = [2, 256, 256]                              # i, x, h\n",
    "MODEL_SHAPE = CROP_SHAPE[-2:] + [CROP_SHAPE[0]]         # x, h, i\n",
    "MODEL_SHAPE_DICE = MODEL_SHAPE + [1]\n",
    "\n",
    "MODEL_SHAPE = tuple(MODEL_SHAPE)\n",
    "MODEL_SHAPE_DICE = tuple(MODEL_SHAPE_DICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "We use smaller version of [one hundred layer Tiramisu](https://arxiv.org/abs/1611.09326) as our base model.\n",
    "\n",
    "As noted before, we add axis to the output of neural network in order to correctly compute Dice-coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(x):\n",
    "    return tf.expand_dims(x, axis=-1, name='expand')\n",
    "\n",
    "# DenseNet config\n",
    "model_config_dense = {\n",
    "                    'inputs': dict(cubes={'shape': MODEL_SHAPE},\n",
    "                                   masks={'name': 'targets', 'shape': MODEL_SHAPE_DICE}), \n",
    "                    'initial_block/inputs': 'cubes',\n",
    "                    'body': {'num_layers': [2]*3,\n",
    "                             'block/growth_rate': 8},\n",
    "                    'loss': 'dice',\n",
    "                    'optimizer': 'Adam',\n",
    "                    'predictions': predictions,\n",
    "                    'output': 'sigmoid',\n",
    "                    'common': {'data_format': 'channels_last'}\n",
    "                     }\n",
    "\n",
    "pipeline_config = {'model': DenseNetFC,\n",
    "                   'model_config': model_config_dense}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline of training:\n",
    "* create positions to cut data from. Note that we use `truncate` method of sampler to sample only from first 80% of ilines\n",
    "* load data from cubes, create segmentation masks\n",
    "* some quality of life augmentations: `rotate axis` so we can think of our crop as of image, `scale` to force cubes to have values in the same range\n",
    "* start training!\n",
    "\n",
    "**Note:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = (Pipeline(config=pipeline_config)\n",
    "                  .load_component(src=[D('geometries'), D('labels')],\n",
    "                                  dst=['geometries', 'labels'])\n",
    "                  .crop(points=L(ds.sampler.truncate(high=0.8, expr=lambda p: p[:, 1]).sample, NUM_CROPS), shape=CROP_SHAPE)\n",
    "                  .load_cubes(dst='data_crops')\n",
    "                  .load_masks(dst='mask_crops')\n",
    "                  .rotate_axes(src=['data_crops', 'mask_crops'])\n",
    "                  .scale(mode='normalize', src='data_crops')\n",
    "                  # Training\n",
    "                  .add_axis(src='mask_crops')\n",
    "                  .init_variable('loss_history', init_on_each_run=list)\n",
    "                  .init_variable('current_loss')\n",
    "                  .init_model('dynamic', C('model'), 'Dense', C('model_config'))\n",
    "                  .train_model('Dense', \n",
    "                               fetches='loss',\n",
    "                               make_data={'cubes': B('data_crops'), 'masks': B('mask_crops')},\n",
    "                               save_to=V('loss_history'), mode='a')) << ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline.run(3, n_epochs=EPOCHS, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how our model performs on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_pipeline.get_variable('loss_history'))\n",
    "plt.xlabel(\"Iterations\"), plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance on unseen part of the cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to slow changes in data along ilines in any given cube, it might be a good idea to test our model against completely new cube. To begin with, we need to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_path_data =           '/notebooks/SEISMIC_DATA/CUBE_2/M_cube.hdf5'\n",
    "test_save_dir =              '/notebooks/tsimfer/SAVED/CUBE_2/'\n",
    "\n",
    "test_path_pc_saved =       test_save_dir + 'point_clouds.dill'   # path_data: point_clouds\n",
    "\n",
    "test_dsi = FilesIndex(path=[test_path_data], no_ext=True)\n",
    "test_ds = SeismicCubeset(test_dsi)\n",
    "\n",
    "test_paths_txt = {test_ds.indices[0]: glob('/notebooks/SEISMIC_DATA/CUBE_2/HORIZONTS/*.txt')}\n",
    "\n",
    "test_ds = (test_ds.load_geometries()\n",
    "                  .load_point_clouds(paths = test_paths_txt)\n",
    "                  .load_labels()\n",
    "                  .load_samplers())\n",
    "\n",
    "## ~ 3 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to use our trained model to detect horizonts. It might be convinient to check not the list of crops, but combine them to predict some bigger part of the cube. That is done via combination of `make_grid` and `assemble_crops`:\n",
    "* `make_grid` takes in ranges of ilines, xlines, height which you want to look at, and created regular grid of points in it. This grid serves as the upper-rightmost points for `crop` action, and we make a prediction for every such crop\n",
    "* `assemble_crops` uses all of the generated predictions to stitch them as one big 3D entity\n",
    "\n",
    "**Note:** `make_grid` takes cube identificator, shape of model inputs (crop_shape) and strides as additional arguments. Strides allow you to overlap crops in order to make better prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_ds = test_ds.make_grid(test_ds.indices[0], CROP_SHAPE, \n",
    "                            [201, 202], [0, 800], [100, 1300],\n",
    "                            strides=[1, 16, 32])\n",
    "\n",
    "print('Shape of grid:', test_ds.grid_info['grid_array'].shape)\n",
    "\n",
    "pred_pipeline = (Pipeline()\n",
    "                 .load_component(src=[D('geometries'), D('labels')],\n",
    "                                 dst=['geometries', 'labels'])\n",
    "                 .crop(points=L(D('grid_gen')),\n",
    "                       shape=CROP_SHAPE)\n",
    "                 .load_cubes(dst='data_crops')\n",
    "                 .create_masks(dst='mask_crops')\n",
    "                 .rotate_axes(src=['data_crops', 'mask_crops'])\n",
    "                 .scale(mode='normalize', src='data_crops')\n",
    "                 .add_axis(src='mask_crops')\n",
    "                 # Predictions\n",
    "                 .import_model('Dense', train_pipeline)\n",
    "                 .init_variable('result_cubes', init_on_each_run=list())\n",
    "                 .init_variable('result_masks', init_on_each_run=list())\n",
    "                 .init_variable('result_preds', init_on_each_run=list())\n",
    "                 .predict_model('Dense', \n",
    "                                fetches=['cubes', 'masks', 'predictions'],\n",
    "                                make_data={'cubes': B('data_crops'), 'masks': B('mask_crops')}, \n",
    "                                save_to=[V('result_cubes'), V('result_masks'), V('result_preds')], mode='e')\n",
    "                 .assemble_crops(src=V('result_cubes'), dst='assembled_cube',\n",
    "                                   grid_info=D('grid_info'), mode='avg')\n",
    "                 .assemble_crops(src=V('result_masks'), dst='assembled_mask',\n",
    "                                   grid_info=D('grid_info'), mode='avg')\n",
    "                 .assemble_crops(src=V('result_preds'), dst='assembled_pred',\n",
    "                                   grid_info=D('grid_info'), mode='max')\n",
    "                 ) << test_ds\n",
    "\n",
    "for _ in range(test_ds.grid_iters):\n",
    "    pred_batch = pred_pipeline.next_batch(1, n_epochs=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
