{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "\n",
    "sys.path.append('..')\n",
    "from seismiqb.batchflow import Dataset, Pipeline, B, V, C, L, F, D, DatasetIndex\n",
    "from seismiqb.batchflow.models.tf import UNet, TFModel\n",
    "from seismiqb import SeismicCropBatch, Geometry, parse_labels, make_histosampler\n",
    "from seismiqb import make_geometries, make_labels, make_samplers\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../../data/Facies/VU_ONGMK/Repaired.sgy'\n",
    "\n",
    "path_label = '../../data/Facies/VU_ONGMK/il_xl_h.dill'\n",
    "path_labels_saved = '../../data/Facies/VU_ONGMK/il_xl_h_model.dill'\n",
    "path_geom_saved = '../../data/Facies/VU_ONGMK/geometry_model.dill'\n",
    "path_samplers_saved = '../../data/Facies/VU_ONGMK/samplers_model.dill'\n",
    "\n",
    "dsi = DatasetIndex([path_data])\n",
    "ds = Dataset(dsi, batch_class = SeismicCropBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# geometries = make_geometries(dataset=ds, save_to=path_geom_saved)\n",
    "geometries = make_geometries(dataset=ds, load_from=path_geom_saved)\n",
    "\n",
    "# labels = make_labels(dataset=ds, load_from=path_label, save_to=path_labels_saved)\n",
    "labels = make_labels(dataset=ds, load_from=path_labels_saved)\n",
    "\n",
    "# samplers = make_samplers(dataset=ds, mode='hist', save_to=path_samplers_saved)\n",
    "samplers = make_samplers(dataset=ds, mode='hist', load_from=path_samplers_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple segmentation\n",
    "model_config_simple = {'inputs': dict(cubes={'shape': (256, 256, 3)},\n",
    "                                      masks={'name': 'targets', 'shape': (256, 256, 3)}),\n",
    "                       'initial_block/inputs': 'cubes',\n",
    "                       'body': dict(layout='cna pd cna pd cna tad tnad tnad t',\n",
    "                                    filters=[4, 8, 16, 8, 4, 2, 3],\n",
    "                                    kernel_size=[7]*7,\n",
    "                                    strides=[2, 2, 1, 2, 2, 2, 2],\n",
    "                                    activation=[tf.nn.elu]*7,\n",
    "                                    pool_size=2, pool_strides=2,\n",
    "                                    dropout_rate=.1),\n",
    "                       'loss': 'mse',\n",
    "                       'optimizer': 'Adam'\n",
    "                        }\n",
    "\n",
    "pipeline_config = {'model': TFModel,\n",
    "                   'model_config': model_config_simple}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(batch, **kwargs):\n",
    "    data_x = []\n",
    "    for cube in batch.data_crops:\n",
    "        cube_ = np.swapaxes(cube, 0, 1)\n",
    "        cube_ = np.swapaxes(cube_, 1, 2)\n",
    "        data_x.append(cube_)\n",
    "        \n",
    "    data_y = []\n",
    "    for cube in batch.mask_crops:\n",
    "        cube_ = np.swapaxes(cube, 0, 1)\n",
    "        cube_ = np.swapaxes(cube_, 1, 2)\n",
    "        data_y.append(cube_.astype(int))\n",
    "    return {\"feed_dict\": {'cubes': data_x,\n",
    "                          'masks': data_y}}\n",
    "\n",
    "\n",
    "train_pipeline = (Pipeline(config=pipeline_config)\n",
    "                  .load_component(src=[D('geometries'), D('labels')],\n",
    "                                  dst=['geometries', 'labels'])\n",
    "                  .crop(points=L(ds.sampler.sample, 64), shape=[3, 256, 256])\n",
    "                  .load_cubes(dst='data_crops')\n",
    "                  .load_masks(dst='mask_crops')\n",
    "                  .init_variable('loss_history', init_on_each_run=list)\n",
    "                  .init_variable('current_loss')\n",
    "                  .init_model('dynamic', C('model'), 'AE', C('model_config'))\n",
    "                  .train_model('AE', \n",
    "                               fetches='loss', \n",
    "                               make_data=make_data,\n",
    "                               save_to=V('current_loss'),\n",
    "                               use_lock=True)\n",
    "                  .update_variable('loss_history', \n",
    "                                   V('current_loss'), \n",
    "                                   mode='a'))\n",
    "\n",
    "train_pipeline = train_pipeline << ds\n",
    "loss_history = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 2\n",
    "\n",
    "train_pipeline.run(batch_size, n_epochs=epochs, drop_last=False, shuffle=False, bar=True)\n",
    "loss_history.extend(train_pipeline.get_variable(\"loss_history\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iterations\"), plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = (Pipeline()\n",
    "                 .load_component(src=[D('geometries'), D('labels')],\n",
    "                                 dst=['geometries', 'labels'])\n",
    "                 .crop(points=L(ds.sampler.sample, 16), shape=[3, 256, 256])\n",
    "                 .load_cubes(dst='data_crops')\n",
    "                 .load_masks(dst='mask_crops')\n",
    "                 .import_model('AE', train_pipeline)\n",
    "                 .init_variable('result', init_on_each_run=list()) \n",
    "                 .predict_model('AE', \n",
    "                                fetches=['predictions', 'cubes', 'masks'],\n",
    "                                make_data=make_data, \n",
    "                                save_to=V('result'), mode='a')\n",
    "                 )\n",
    "\n",
    "test_pipeline = (test_pipeline) << ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_pipeline.next_batch(batch_size=1, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cube in range(4):\n",
    "    iline = 1\n",
    "    predicted_mask = test_pipeline.get_variable('result')[0][0][cube, :, :, iline].T\n",
    "    img = test_pipeline.get_variable('result')[0][1][cube, :, :, iline].T\n",
    "    masks = test_pipeline.get_variable('result')[0][2][cube, :, :, iline].T\n",
    "\n",
    "    cv = 0.1\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "\n",
    "    ax[0].imshow(masks, cmap=\"Greens\")\n",
    "    ax[0].imshow(img, vmin=-cv, vmax=cv, cmap=\"gray\", alpha=0.2)\n",
    "    ax[0].set_title('Initial mask')\n",
    "\n",
    "    ax[1].imshow(predicted_mask, cmap=\"Greens\")\n",
    "    ax[1].imshow(img, vmin=-cv, vmax=cv, cmap=\"gray\", alpha=0.2)\n",
    "\n",
    "\n",
    "    ax[1].set_title('Predicted mask')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch.slice_points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_iline(iline, pipeline, name, cube_name, shape):\n",
    "    geom = pipeline.dataset.geometries[cube_name]\n",
    "    x_ticks = geom.xlines_len // shape[1]\n",
    "    h_ticks = geom.depth // shape[2]        \n",
    "    \n",
    "    points = []\n",
    "    for x in range(x_ticks):\n",
    "        for h in range(h_ticks):\n",
    "            point = [cube_name,\n",
    "                     (iline + shape[0]//2)/geom.ilines_len,\n",
    "                     x * shape[1]/geom.xlines_len,\n",
    "                     h * shape[2]/geom.depth]\n",
    "            points.append(point)\n",
    "    points = np.array(points, dtype=object)\n",
    "\n",
    "    pred_pipeline = (Pipeline()\n",
    "                     .load_component(src=[D('geometries'), D('labels')],\n",
    "                                     dst=['geometries', 'labels'])\n",
    "                     .crop(points=points, shape=shape)\n",
    "                     .load_cubes(dst='data_crops')\n",
    "                     .load_masks(dst='mask_crops')\n",
    "                     .import_model(name, pipeline)\n",
    "                     .init_variable('result', init_on_each_run=list()) \n",
    "                     .predict_model(name, \n",
    "                                    fetches=['predictions', 'cubes', 'masks'],\n",
    "                                    make_data=make_data, \n",
    "                                    save_to=V('result'), mode='a')\n",
    "                     )\n",
    "    \n",
    "    pred_pipeline = (pred_pipeline) << ds\n",
    "    pred_batch = pred_pipeline.next_batch(batch_size=1, n_epochs=1)\n",
    "    \n",
    "    big_img = np.zeros((x_ticks*shape[1], h_ticks*shape[2]))\n",
    "    big_mask = np.zeros((x_ticks*shape[1], h_ticks*shape[2]))\n",
    "    big_pred = np.zeros((x_ticks*shape[1], h_ticks*shape[2]))\n",
    "    \n",
    "    for i, point in enumerate(points):\n",
    "        point_ = np.array([point[2]*(geom.xlines_len - shape[1]),\n",
    "                           point[3]*(geom.depth - shape[2])]).astype(int)\n",
    "        \n",
    "        img = pred_pipeline.get_variable('result')[0][1][i, :, :, 0]\n",
    "        mask = pred_pipeline.get_variable('result')[0][2][i, :, :, 0]\n",
    "        predicted_mask = pred_pipeline.get_variable('result')[0][0][i, :, :, 0]\n",
    "\n",
    "        big_img[point_[0]: point_[0]+shape[1], point_[1]: point_[1]+shape[2]] = img\n",
    "        big_mask[point_[0]: point_[0]+shape[1], point_[1]: point_[1]+shape[2]] = mask\n",
    "        big_pred[point_[0]: point_[0]+shape[1], point_[1]: point_[1]+shape[2]] = predicted_mask\n",
    "\n",
    "    \n",
    "    cv = 0.1\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 18))\n",
    "\n",
    "    ax[0].imshow(big_mask.T, cmap=\"Greens\")\n",
    "    ax[0].imshow(big_img.T, vmin=-cv, vmax=cv, cmap=\"gray\", alpha=0.2)\n",
    "    ax[0].set_title('Initial mask')\n",
    "    \n",
    "    ax[1].imshow(big_pred.T, cmap=\"Greens\")\n",
    "    ax[1].imshow(big_img.T, vmin=-cv, vmax=cv, cmap=\"gray\", alpha=0.2)\n",
    "    ax[1].set_title('Predicted mask')\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch = predict_on_iline(550, train_pipeline, 'AE', path_data, [3, 256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelShuffle(UNet):\n",
    "    @classmethod\n",
    "    def decoder_block(cls, inputs, filters, upsample=None, decoder=None, name='decoder', **kwargs):\n",
    "        upsample = cls.fill_params('body/upsample', **upsample)\n",
    "        decoder = cls.fill_params('body/decoder', **decoder)\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            x, skip = inputs\n",
    "            inputs = None\n",
    "            x = conv_block(x, layout='Cna', kernel_size=3,\n",
    "                            filters=filters*4, strides=1, activation=tf.nn.elu,\n",
    "                            name='prepix')\n",
    "            x = tf.nn.depth_to_space(x, block_size=2, name='pixel_shuffle')\n",
    "#             x = cls.upsample(x, filters=filters, name='upsample', **{**kwargs, **upsample})\n",
    "            x = cls.crop(x, skip, data_format=kwargs.get('data_format'))\n",
    "            axis = cls.channels_axis(kwargs.get('data_format'))\n",
    "            x = tf.concat((skip, x), axis=axis)\n",
    "            x = conv_block(x, filters=filters, name='conv', **{**kwargs, **decoder})\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel-shuffle\n",
    "model_config_ps = {\n",
    "                    'inputs': dict(cubes={'shape': (256, 256, 3)},\n",
    "                                   masks={'name': 'targets', 'shape': (256, 256, 3)}), \n",
    "                    'initial_block/inputs': 'cubes',\n",
    "                    'body/filters': [8, 16, 32, 64],\n",
    "                    'body/encoder': dict(layout='cna', kernel_size=3, activation=tf.nn.elu),\n",
    "                    'body/downsample': dict(layout='pd', pool_size=2, pool_strides=2, dropout_rate=0.05),\n",
    "                    'body/decoder': dict(layout='cna', kernel_size=3, activation=tf.nn.elu),\n",
    "                    'body/upsample': dict(layout='tad', kernel_size=3, strides=2,\n",
    "                                          dropout_rate=0.05, activation=tf.nn.elu),\n",
    "                    'loss': 'mse',\n",
    "                    'optimizer': 'Adam'\n",
    "                     }\n",
    "\n",
    "pipeline_config = {'model': PixelShuffle,\n",
    "                   'model_config': model_config_ps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(batch, **kwargs):\n",
    "    data_x = []\n",
    "    for cube in batch.data_crops:\n",
    "        cube_ = np.swapaxes(cube, 0, 1)\n",
    "        cube_ = np.swapaxes(cube_, 1, 2)\n",
    "        data_x.append(cube_)\n",
    "        \n",
    "    data_y = []\n",
    "    for cube in batch.mask_crops:\n",
    "        cube_ = np.swapaxes(cube, 0, 1)\n",
    "        cube_ = np.swapaxes(cube_, 1, 2)\n",
    "        data_y.append(cube_*10)\n",
    "    return {\"feed_dict\": {'cubes': data_x,\n",
    "                          'masks': data_y}}\n",
    "\n",
    "\n",
    "train_pipeline = (Pipeline(config=pipeline_config)\n",
    "                  .load_component(src=[D('geometries'), D('labels')],\n",
    "                                  dst=['geometries', 'labels'])\n",
    "                  .crop(points=L(ds.sampler.sample, 64), shape=[3, 256, 256])\n",
    "                  .load_cubes(dst='data_crops')\n",
    "                  .load_masks(dst='mask_crops')\n",
    "                  .init_variable('loss_history', init_on_each_run=list)\n",
    "                  .init_variable('current_loss')\n",
    "                  .init_model('dynamic', C('model'), 'AE', C('model_config'))\n",
    "                  .train_model('AE', \n",
    "                               fetches='loss', \n",
    "                               make_data=make_data,\n",
    "                               save_to=V('current_loss'),\n",
    "                               use_lock=True)\n",
    "                  .update_variable('loss_history', \n",
    "                                   V('current_loss'), \n",
    "                                   mode='a'))\n",
    "\n",
    "train_pipeline = train_pipeline << ds\n",
    "loss_history = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 5\n",
    "\n",
    "train_pipeline.run(batch_size, n_epochs=epochs, drop_last=False, shuffle=False, bar=True)\n",
    "loss_history.extend(train_pipeline.get_variable(\"loss_history\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
